{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from langdetect import detect\n",
    "import re\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.select import Select\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving Top 100 Book Titles and Links to Webpage for Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating array from Times Top 100 Books of All Time webpage on Goodreads website\n",
    "\n",
    "TimesTop100  = requests.get(\"https://www.goodreads.com/list/show/2681.Time_Magazine_s_All_Time_100_Novels\").content\n",
    "\n",
    "## Grabbing all tags in webpage of 'a' type and class 'bookTitle'\n",
    "soup = BeautifulSoup(TimesTop100,\"lxml\")\n",
    "cont = soup.select(\"a.bookTitle\")\n",
    "\n",
    "## Iterating through and creating list for all titles (bookT) and links (bookLink)\n",
    "bookT = [x.text.strip() for x in cont]\n",
    "bookLink = ['https://www.goodreads.com'+ x.get('href') for x in cont]\n",
    "\n",
    "## Combining list\n",
    "con = np.column_stack((bookT, bookLink))\n",
    "Top100Books = pd.DataFrame(con, columns = ['Book Title', 'Book Link'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving Book Review Page Links and Unqiue ID for Book in Goodreads Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving Page Link for the reviews of a certain book\n",
    "# requests.get has tendency to crash so must be run with a while loop until requests successfully works - load time (~11.5 min)\n",
    "\n",
    "ReviewPageLink = []\n",
    "for i in bookLink:\n",
    "    cont = None\n",
    "    while cont == None:\n",
    "            r = requests.get(i).content\n",
    "            soup = BeautifulSoup(r,\"lxml\")\n",
    "            cont = soup.find(\"a\", attrs = {'class' : 'Button Button--transparent Button--small'})\n",
    "\n",
    "    ReviewPageLink += ['https://www.goodreads.com' + cont['href']]\n",
    "\n",
    "# Finding the Unique ID set by Goodreads in each books main page href\n",
    "BookID = [re.search('\\d+', i)[0] for i in Top100Books['Book Link']]\n",
    "\n",
    "# adding information to dataframe\n",
    "Top100Books['Review Page Link'] = ReviewPageLink\n",
    "Top100Books['Unique ID'] = BookID\n",
    "\n",
    "# saving dataframe to csv file\n",
    "# Top100Books.to_csv('Top100BooksData.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining function to continuing spamming requests until html can be processed\n",
    "## when looking to iterate through multiple links while scraping\n",
    "\n",
    "# Code currently returns a search - however - might be more important to return just the soup \n",
    "# Therefore, it can be used for mulitple searches if required\n",
    "# The caveat is that by the definition of the function it must find some object that does exist on the page regardless\n",
    "# in order to work \n",
    "\n",
    "def spamRequestsFind(link, tag, attr, attr_id ):\n",
    "    status = None\n",
    "    while status == None:\n",
    "        r = requests.get(link).content\n",
    "        soup = BeautifulSoup(r, \"lxml\")\n",
    "        status = type(soup.find(tag, attrs = {attr : attr_id}))\n",
    "\n",
    "    return(soup.find(tag, attrs = {attr : attr_id}))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book Title</th>\n",
       "      <th>Book Link</th>\n",
       "      <th>Review Page Link</th>\n",
       "      <th>Unique ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>https://www.goodreads.com/book/show/2657.To_Ki...</td>\n",
       "      <td>https://www.goodreads.com/book/show/2657/revie...</td>\n",
       "      <td>2657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1984</td>\n",
       "      <td>https://www.goodreads.com/book/show/5470.1984</td>\n",
       "      <td>https://www.goodreads.com/book/show/5470/revie...</td>\n",
       "      <td>5470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Lord of the Rings</td>\n",
       "      <td>https://www.goodreads.com/book/show/33.The_Lor...</td>\n",
       "      <td>https://www.goodreads.com/book/show/33/reviews...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Catcher in the Rye</td>\n",
       "      <td>https://www.goodreads.com/book/show/5107.The_C...</td>\n",
       "      <td>https://www.goodreads.com/book/show/5107/revie...</td>\n",
       "      <td>5107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Great Gatsby</td>\n",
       "      <td>https://www.goodreads.com/book/show/4671.The_G...</td>\n",
       "      <td>https://www.goodreads.com/book/show/4671/revie...</td>\n",
       "      <td>4671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Book Title                                          Book Link  \\\n",
       "0   To Kill a Mockingbird  https://www.goodreads.com/book/show/2657.To_Ki...   \n",
       "1                    1984      https://www.goodreads.com/book/show/5470.1984   \n",
       "2   The Lord of the Rings  https://www.goodreads.com/book/show/33.The_Lor...   \n",
       "3  The Catcher in the Rye  https://www.goodreads.com/book/show/5107.The_C...   \n",
       "4        The Great Gatsby  https://www.goodreads.com/book/show/4671.The_G...   \n",
       "\n",
       "                                    Review Page Link  Unique ID  \n",
       "0  https://www.goodreads.com/book/show/2657/revie...       2657  \n",
       "1  https://www.goodreads.com/book/show/5470/revie...       5470  \n",
       "2  https://www.goodreads.com/book/show/33/reviews...         33  \n",
       "3  https://www.goodreads.com/book/show/5107/revie...       5107  \n",
       "4  https://www.goodreads.com/book/show/4671/revie...       4671  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Top100Books = pd.read_csv('Top100BooksData.csv')\n",
    "Top100Books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MultBookReviews grabs all defined information for all book reviews loaded on the page in browser\n",
    "\n",
    "def MultBookReviews(page_source):\n",
    "\n",
    "    ## starting by grabbing one persons review and information\n",
    "    \n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "    ReviewCards = soup.find_all('article', attrs = {'class' : 'ReviewCard'})\n",
    "\n",
    "    ## Book Title and Author\n",
    "    title = soup.find('h1', attrs = {'class' : 'Text H1Title'}).text\n",
    "    author = soup.find('h3', attrs = {'class' : 'Text Text__title3 Text__regular'}).text\n",
    "\n",
    "    ## List of all user account hrefs for account page\n",
    "    cont = soup.select('div.ReviewerProfile__name')\n",
    "    hrefsUsers = [x.find('a')['href'] for x in cont]\n",
    "\n",
    "    ## Text data of user review\n",
    "    contReview = soup.select(\"section.ReviewText\")\n",
    "    Reviews = [x.text.strip() for x in contReview]\n",
    "\n",
    "    ## grabbing individual user rating for review \n",
    "    contRatingCont = soup.select(\"div.ShelfStatus\")\n",
    "    userRatings = [x.find('span')['aria-label'] if (x.findChildren('span', recursive=False) == []) == False else 'No Rating' for x in contRatingCont]\n",
    "\n",
    "    ## Date the review was written by user\n",
    "    dateCont = soup.select('section.ReviewCard__row')\n",
    "    datesOfReviews = [x.find('span', attrs = {'class': 'Text Text__body3'}).text for x in dateCont]\n",
    "\n",
    "    ## Amount of likes and comments for review\n",
    "    commentLikeCont = soup.select('footer.SocialFooter')\n",
    "    likes = ['0' if x.find('div', attrs={'class': 'SocialFooter__statsContainer'}) == None else x.find('span', attrs={'class': 'Button__labelItem'}).text  for x in commentLikeCont]\n",
    "    comments = ['0' if x.find('div', attrs={'class': 'Button__container'}).next_sibling == None else x.find('div', attrs={'class': 'Button__container'}).next_sibling.text for x in commentLikeCont]\n",
    "\n",
    "    ### Creating DataFrame of all the review data\n",
    "\n",
    "    reviewData = pd.DataFrame({ 'User Href' : hrefsUsers,\n",
    "                                'Title' : title,\n",
    "                                'Rating' : userRatings,\n",
    "                                'Date' : datesOfReviews,\n",
    "                                'Likes' : likes,\n",
    "                                'Comments' : comments,\n",
    "                                'Review' : Reviews})\n",
    "    \n",
    "    return(reviewData)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Function to Automate clicking \"show more reviews\" on each book webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getManyReviews uses selenium to load more reviews on a book review webpage\n",
    "## Each iteration clicks the \"show more reviews\" button (loads 30 more reviews per click) up to a cap of 36 clicks\n",
    "## 36 was determined as the cap before the memory of a browser page would be expended resulting in a crash\n",
    "\n",
    "def getManyReviews(url):\n",
    "    clicks = 0\n",
    "\n",
    "    # Initilaizing driver and webpage and allowing time for reviews to load\n",
    "    driver = webdriver.Edge()\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "    nreviews = int(re.sub('\\D', '', soup.find('span', attrs = {'class' : 'Text Text__body3 Text__subdued'}).text))\n",
    "    cap = 36\n",
    "    iters = np.round(nreviews/30)-1\n",
    "\n",
    "    if iters < cap:\n",
    "        while clicks < iters:\n",
    "\n",
    "            # scrolling down page to ensure click will work on \"show more results\" button\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            # Clicking \"show more results\" button\n",
    "            SMResults = driver.find_element(By.XPATH, \"//div[@class = 'Divider Divider--contents Divider--largeMargin']/div[@class = 'Button__container']/button\")\n",
    "            driver.execute_script(\"arguments[0].click();\", SMResults)\n",
    "            time.sleep(1)\n",
    "\n",
    "            clicks += 1\n",
    "    else:\n",
    "        while clicks < cap:\n",
    "\n",
    "            # scrolling down page to ensure click will work on \"show more results\" button\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            # Clicking \"show more results\" button\n",
    "            SMResults = driver.find_element(By.XPATH, \"//div[@class = 'Divider Divider--contents Divider--largeMargin']/div[@class = 'Button__container']/button\")\n",
    "            driver.execute_script(\"arguments[0].click();\", SMResults)\n",
    "            time.sleep(1)\n",
    "\n",
    "            clicks += 1\n",
    "\n",
    "    # grabbing reference for final state of page after n number of \"show more results\" button clicks\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    reviews = MultBookReviews(page_source)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return(reviews)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterating Through Batches of Books to Grab Reviews (5 books, ~1000 reviews per book, ~15 min to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing in smaller batches to double check loading into csv was successful\n",
    "for i in range(0, 5):\n",
    "    bookdata = getManyReviews(Top100Books['Review Page Link'][i])\n",
    "    bookdata.to_csv('Book{num}.csv'.format(num=i+1), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
