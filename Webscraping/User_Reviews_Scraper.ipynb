{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from googletrans import Translator\n",
    "\n",
    "from model_dependency_script import text_preprocessing, sparse_to_dense\n",
    "import joblib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will need to ensure **model_dependency_script** and **joblib** files are located in same file that this notebook is running in order to properly load"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Retrieve User Review Page Links\n",
    "Input : Book Reviews dataframe\n",
    "- Extracts user_id and display_name\n",
    "- Then concatenates them into the string format for goodreads user review pages\n",
    "\n",
    "Output : list, containing user review page links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_revpage_href(br):\n",
    "    userID_Disp = br[['user_id', 'display_name']].reset_index(drop=True)\n",
    "    duplicates = userID_Disp.duplicated(subset=['user_id'], keep=False)\n",
    "    clean_df = userID_Disp[~duplicates]\n",
    "\n",
    "    # Creates list of all user review page hrefs\n",
    "    userReviewPagehref  = ['https://www.goodreads.com' + '/review/list/{}-{}?order=d&sort=review&view=reviews'.format(clean_df['user_id'].iloc[i], clean_df['display_name'].iloc[i]) for i, x in enumerate(clean_df['user_id'])]\n",
    "    return(userReviewPagehref)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Scroll through User Review \n",
    "Input: user review page link\n",
    "- Firstly, it ensures the page is in fact a review page by checking the arguments in the url after loading it via the headless browser\n",
    "- Secondly, if the page is valid, the total amount of reviews are determined to see if the amount of information to scroll through will surpass what is scrapable\n",
    "- Lastly, the page is scrolled through until all reviews or maximum reviews possible are loaded and the html for the final result of the page is returned\n",
    "\n",
    "Output: loaded html containing all (or maximum potential to be scraped) reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scroll_for_Reviews_getHTML(link):\n",
    "    \n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--ignore-certificate-errors')\n",
    "    chrome_options.add_argument('--ignore-ssl-errors')\n",
    "    chrome_options.add_argument('--headless')\n",
    "    service = Service(r\"C:\\Users\\marty\\OneDrive - The George Washington University\\Documents\\CSCI 4443\\Project\\chromedriver_win32\\chromedriver.exe\")\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    \n",
    "    # Load goodreads page\n",
    "    driver.get(link)\n",
    "    time.sleep(3)\n",
    "\n",
    "    current_url = driver.current_url\n",
    "\n",
    "    # checking to see if page has proper argument in html defining it to be an accessible review page\n",
    "    if 'order=d&sort=review&view=reviews' in current_url:\n",
    "\n",
    "        # Grabbing total number of reviews on page text\n",
    "        totalReviews = driver.find_element(By.XPATH, \"//div[@class = 'buttons clearFloats uitext']/div[@id = 'infiniteStatus']\")\n",
    "        text = totalReviews.text\n",
    "\n",
    "        # taking text and finding only digits surrounded by spaces then converting to int type\n",
    "        totrevs = int(re.findall(r'(?<=\\s)\\d+(?=\\s)', text)[0])\n",
    "\n",
    "        # Determining if page has over 1200 reviews or not\n",
    "        # if so, scrolling to bottom of page is capped once 1200 reviews are loaded\n",
    "        # else, scrolls until bottom of page\n",
    "\n",
    "        if totrevs > 1200:\n",
    "            for i in range(38):\n",
    "                # Scroll down to the bottom\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "                # Wait to load the page\n",
    "                time.sleep(1.5)\n",
    "\n",
    "        else:\n",
    "            # Get initial page height\n",
    "            last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "            while True:\n",
    "                # Scroll down to the bottom\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "                # Wait to load the page\n",
    "                time.sleep(1.5)\n",
    "\n",
    "                # Calculate new page height and compare with last height\n",
    "                new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                if new_height == last_height:\n",
    "                    # If heights are the same, there is no more content to load\n",
    "                    break\n",
    "                last_height = new_height\n",
    "\n",
    "        # retrieve the html in its fully loaded state\n",
    "        html = driver.page_source\n",
    "\n",
    "        driver.quit()\n",
    "        return(html)\n",
    "    \n",
    "    else:\n",
    "        driver.quit()\n",
    "        return(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Retrieves User Reviews for the loaded html retrieved from Scroll_for_Reviews_getHTML()\n",
    "Input: Loaded HTML\n",
    "- uses BeautifulSoup to create bs4.elements from loaded html and searches for the relevant user information\n",
    "\n",
    "Output: pandas dataframe, containing user reviews (and relevant information related)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_info_fromHTML(html, link):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # grabbing all tr tags under tbody tag that each house one review\n",
    "    revWrapper = soup.find('tbody', attrs = {'id' : 'booksBody'}).find_all('tr')\n",
    "\n",
    "    # fetching user ID\n",
    "    user_ID = [re.findall('\\d+', link)[0] for i in revWrapper]\n",
    "\n",
    "    # grabbing each reviews ID\n",
    "    review_ID = [revWrapper[i]['id'][7:] for i,x in enumerate(revWrapper)]\n",
    "\n",
    "    # grabbing book titles \n",
    "    book_title = [revWrapper[i]('td')[3]('a')[0]['title'] for i,x in enumerate(revWrapper)]\n",
    "\n",
    "    # grabbing author name\n",
    "    author_name = [revWrapper[i]('td')[4]('a')[0].text for i,x in enumerate(revWrapper)]\n",
    "\n",
    "    # grabbing rating \n",
    "    book_rating = [revWrapper[i]('td')[9]('div')[0].text.strip() for i,x in enumerate(revWrapper)]\n",
    "\n",
    "    # grabbing review \n",
    "    book_review = [revWrapper[i]('td')[15]('div')[0].text for i,x in enumerate(revWrapper)]\n",
    "\n",
    "    # grabbing date added\n",
    "    date_added = [revWrapper[i]('td')[22]('div')[0].text.strip() for i,x in enumerate(revWrapper)]\n",
    "\n",
    "    # creating if-else where if first iteration, creates initial array, arr\n",
    "    # and when it is a successive iteration, it appends this iterations data to the intial array\n",
    "    arr = np.column_stack((  user_ID, review_ID, book_title, author_name, \n",
    "                             book_rating, book_review, date_added  ))\n",
    "        \n",
    "    df = pd.DataFrame(arr, columns = ['User ID', 'Review ID', 'Title', 'Author', 'Rating', 'Review', 'Date Added'])\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Combines Scroll_for_Reviews_getHTML() and retrieve_info_fromHTML() Into One \n",
    "- This streamlines the process of running in the notebook \n",
    "- If Scroll_for_Reviews_getHTML() found a private user (or other similar condition) the html will return as zero and not be processed as not reviews are scrapable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUserReviews(link):\n",
    "    html = Scroll_for_Reviews_getHTML(link)\n",
    "\n",
    "    if html == 0:\n",
    "        return(None)\n",
    "    else:\n",
    "        return(retrieve_info_fromHTML(html, link))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating through list of users with flagged spam reviews determined Classifer \n",
    "Input : user_hrefs_list (list of user review pages links created from flagged_spammers.csv)\n",
    "- flagged_spammers.csv is a file with saved user_id, display_name who users who were flagged as spam by the spam_classifier\n",
    "- This file was created by saving the results from the arango query of the goodreads database collection, spam_reviews\n",
    "\n",
    "- The loop works by maintaining a current state (index) of how many users have been scraped (and or attempted) and then appending to the csv file, spammers_reviews\n",
    "- every iteration creates a small list of links, linklst, whose elements are subsection of user_hrefs_list where the index is [state:state+niters]\n",
    "- These are then used when calling the function, getUserReviews()\n",
    "\n",
    "Output : spammers_reviews.csv file (contains book reviews made by flagged users if they are not private)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting state for cell below to run loop, do not reset until finished iterating through following cell\n",
    "state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting number of iterations, then load list of userhrefs, revs\n",
    "# linklst is a sample taken from the top of revs to grab niters of links to cycle through\n",
    "\n",
    "niters = 5\n",
    "\n",
    "# loading dataframe containing flagged spammers user_id and display_name\n",
    "flagged_spammers = pd.read_csv('flagged_spammers.csv')\n",
    "\n",
    "# intializing name of file to save scraped reviews to\n",
    "filename = 'spammer_reviews.csv'\n",
    "\n",
    "# creating list of links to user review pages\n",
    "user_hrefs_list = get_user_revpage_href(flagged_spammers)\n",
    "\n",
    "linklst = user_hrefs_list[state:state+niters]\n",
    "\n",
    "# iterating for each unique user href to get reviews if available (not if user is private)\n",
    "for link in linklst:\n",
    "    revs = getUserReviews(link)\n",
    "    if revs is None:\n",
    "        continue\n",
    "    elif isinstance(revs, pd.DataFrame):\n",
    "        if os.path.isfile(filename):\n",
    "            revs.to_csv(filename, index=False, mode='a', header=False)\n",
    "        else:\n",
    "            revs.to_csv(filename, index=False, header=True)\n",
    "\n",
    "state += niters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights\n",
    "Initially, the flagged users from the arango query results numbered 134. However, upon inspection of duplicates it was found 3 users had reviewed at least 2 of the 3 books that the spam_classifer was tested on. This lowered the unique users flagged for spam down to 131.\n",
    "\n",
    "The selection also contained 44 private users, leading to the final file containing reviews for 87 out of the 131 flagged users"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading saved spammer_review file from previous scrape and cleaning before post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading saved spammer_reviews file after finishing the scrape\n",
    "spammer_reviews = pd.read_csv(r\"C:\\Users\\marty\\OneDrive - The George Washington University\\Documents\\Applied Machine Learning Analytics\\Project\\spammer_reviews.csv\")\n",
    "\n",
    "# cleaning column names\n",
    "columns_spam_book_reviews = [i.lower().replace(' ', '_') for i in list(spammer_reviews.keys())]\n",
    "\n",
    "for i in range(len(columns_spam_book_reviews)):\n",
    "    spammer_reviews = spammer_reviews.rename(columns = {spammer_reviews.columns[i]: columns_spam_book_reviews[i]})\n",
    "\n",
    "# creating list of reviews where any review that has not text is set to '123'\n",
    "# this will allow language detection to properly identify it as 'unknown'\n",
    "\n",
    "noNones = []\n",
    "\n",
    "for i, x in enumerate(spammer_reviews['review']):\n",
    "    if x == '\\nNone\\n\\n':\n",
    "        noNones += ['123']\n",
    "    else:\n",
    "        noNones += [x]\n",
    "\n",
    "spammer_reviews['review'] = noNones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the language of each review and creating column in dataframe that houses outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text, default_language='unknown'):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "    except LangDetectException as e:\n",
    "        if \"No features in text\" in str(e):\n",
    "            language = default_language\n",
    "        else:\n",
    "            raise e\n",
    "    return language\n",
    "\n",
    "lang = [detect_language(spammer_reviews['review'].iloc[i]) for i,x in enumerate(spammer_reviews['review'])]\n",
    "spammer_reviews['lang'] = lang"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating through spammer_reviews and translating reviews to english\n",
    "- only initialize cell below once and then continue to run second cell to completion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing state of iterations\n",
    "translated_rev = []\n",
    "state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definining number of reviews, niters, to translate per cell run\n",
    "translator = Translator()\n",
    "niters = 2000\n",
    "\n",
    "# looping through to append translated reviews to the list, translated_rev\n",
    "lst = spammer_reviews['review'].iloc[state:niters+state]\n",
    "for text in lst:\n",
    "    translated_rev += [translator.translate(text, dest='en').text]\n",
    "\n",
    "state += niters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing original reviews with english translation\n",
    "spammer_reviews['review'] = translated_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the complete dataset\n",
    "spammer_reviews_c = pd.read_csv(r\"C:\\Users\\marty\\OneDrive - The George Washington University\\Documents\\Applied Machine Learning Analytics\\Project\\spammer_reviews_c.csv\").drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving version of the file without empty reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving version of spammer_reviews that is processed and with no reviews that have no text\n",
    "full_revs = spammer_reviews_c[spammer_reviews_c['lang'] != 'unknown']\n",
    "full_revs.to_csv(r\"C:\\Users\\marty\\OneDrive - The George Washington University\\Documents\\Applied Machine Learning Analytics\\Project\\spammer_reviews_cfull.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Spam Classifier to all applicable reviews from spammer_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv(r\"C:\\Users\\marty\\OneDrive - The George Washington University\\Documents\\Applied Machine Learning Analytics\\Project\\spammer_reviews_cfull.csv\").drop('Unnamed: 0', axis=1)\n",
    "spam_detector = joblib.load('spam_detector.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "\n",
    "# applying label to each review, in the case of a review that possess no text features (ex. '5.0') a label of 'n/a' is applied\n",
    "for review in a['review']:\n",
    "    try:\n",
    "        label += [spam_detector.predict(review)]\n",
    "    except LangDetectException as e:\n",
    "        if \"No features in text\" in str(e):\n",
    "            label += ['n/a']\n",
    "        else:\n",
    "            raise e\n",
    "        \n",
    "# intializing column in dataframe for spam labels\n",
    "a['spam'] = label\n",
    "a['spam'] = a['spam'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>date_added</th>\n",
       "      <th>lang</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6470167</td>\n",
       "      <td>500297253</td>\n",
       "      <td>أثر الفراشة</td>\n",
       "      <td>Darwish, Mahmoud</td>\n",
       "      <td>4.03</td>\n",
       "      <td>[image]\\nDid you know that there is a scientif...</td>\n",
       "      <td>Jan 08, 2013</td>\n",
       "      <td>ar</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6470167</td>\n",
       "      <td>483767778</td>\n",
       "      <td>مشكلة الأفكار في العالم الإسلامي</td>\n",
       "      <td>ابن نبي, مالك</td>\n",
       "      <td>4.10</td>\n",
       "      <td>\\nلا ينقضي عجبي بعد أنهيت هذا الكتاب ، فحينما ...</td>\n",
       "      <td>Dec 23, 2012</td>\n",
       "      <td>ar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6470167</td>\n",
       "      <td>513086746</td>\n",
       "      <td>زمن الخيول البيضاء</td>\n",
       "      <td>Nasrallah, Ibrahim</td>\n",
       "      <td>4.43</td>\n",
       "      <td>\\n\\n  [image]\\nزمن الخيول البيضاء..لا أعلم كيف...</td>\n",
       "      <td>Jan 21, 2013</td>\n",
       "      <td>ar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6470167</td>\n",
       "      <td>254394165</td>\n",
       "      <td>رياض الصالحين</td>\n",
       "      <td>النووي, يحيى بن شرف</td>\n",
       "      <td>4.65</td>\n",
       "      <td>May God have mercy on the learned and ascetic ...</td>\n",
       "      <td>Jan 01, 2012</td>\n",
       "      <td>ar</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6470167</td>\n",
       "      <td>359020135</td>\n",
       "      <td>الطريق إلى القرآن</td>\n",
       "      <td>السكران, إبراهيم عمر</td>\n",
       "      <td>4.24</td>\n",
       "      <td>[image]\\n“The path to the Qur’an”.... its name...</td>\n",
       "      <td>Jul 01, 2012</td>\n",
       "      <td>ar</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  review_id                             title                author  \\\n",
       "0  6470167  500297253                       أثر الفراشة      Darwish, Mahmoud   \n",
       "1  6470167  483767778  مشكلة الأفكار في العالم الإسلامي         ابن نبي, مالك   \n",
       "2  6470167  513086746                زمن الخيول البيضاء    Nasrallah, Ibrahim   \n",
       "3  6470167  254394165                     رياض الصالحين   النووي, يحيى بن شرف   \n",
       "4  6470167  359020135                 الطريق إلى القرآن  السكران, إبراهيم عمر   \n",
       "\n",
       "   rating                                             review    date_added  \\\n",
       "0    4.03  [image]\\nDid you know that there is a scientif...  Jan 08, 2013   \n",
       "1    4.10  \\nلا ينقضي عجبي بعد أنهيت هذا الكتاب ، فحينما ...  Dec 23, 2012   \n",
       "2    4.43  \\n\\n  [image]\\nزمن الخيول البيضاء..لا أعلم كيف...  Jan 21, 2013   \n",
       "3    4.65  May God have mercy on the learned and ascetic ...  Jan 01, 2012   \n",
       "4    4.24  [image]\\n“The path to the Qur’an”.... its name...  Jul 01, 2012   \n",
       "\n",
       "  lang spam  \n",
       "0   ar    0  \n",
       "1   ar    1  \n",
       "2   ar    1  \n",
       "3   ar    0  \n",
       "4   ar    0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the final dataset from saving the dataframe processes in previous cell\n",
    "a = pd.read_csv(r\"C:\\Users\\marty\\OneDrive - The George Washington University\\Documents\\Applied Machine Learning Analytics\\Project\\spammer_reviews_cdone.csv\").drop('Unnamed: 0', axis =1)\n",
    "a.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File Recap\n",
    "- spammer_reviews.csv : original scraped dataset of flagged users spam reviews\n",
    "- spammer_reviews_c.csv : dataset with translated reviews and additional column with label for the language of the original review\n",
    "- spammer_reviews_cfull.csv : dataset striped of any empty reviews \n",
    "- spammer_reviews_cdone.csv : dataset with additional column with spam label applied via spam_detector.joblib\n",
    "\n",
    "spammer_reviews_cdone.csv is the final copy for upload into the arango database\n",
    "\n",
    "File versions are maintained in an effort to keep whole dataset at each step if further analysis proves useful"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
