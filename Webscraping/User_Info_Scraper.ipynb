{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import choice\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading review data\n",
    "br = pd.read_csv('../BookReviews.csv').dropna()\n",
    "br = br.reset_index(drop=True)\n",
    "\n",
    "## creating numpy array of review data with only unique user href's\n",
    "br_u = br['User Href'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining function to continuing spamming requests until html can be processed\n",
    "## when looking to iterate through multiple links while scraping\n",
    "\n",
    "# returns a soup and request\n",
    "\n",
    "# The caveat is that by the definition, the function must find some object \n",
    "# that exists on the page in order to work \n",
    "\n",
    "def spamRequestsFind(link, tag, attr, attr_id):\n",
    "    status = None\n",
    "    while status == None:\n",
    "        r = requests.get(link)\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "        status = type(soup.find(tag, attrs = {attr : attr_id}))\n",
    "\n",
    "    prefix = len('https://www.goodreads.com/user/show/')\n",
    "    userID_disp = link[prefix:]\n",
    "    match = re.match(r'^(\\d+)-', userID_disp)\n",
    "    if match:\n",
    "        userID = match.group(1)\n",
    "    return(soup, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function is necessary to retrieve an updated url for users whose page url link has undergone changes\n",
    "## this occurs in the instance of existing user becoming a goodreads verified author\n",
    "\n",
    "def updated_url_html(r):\n",
    "    # Set up the Selenium driver with the path to the ChromeDriver executable\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    service = Service(r\"C:\\Users\\marty\\OneDrive - The George Washington University\\Documents\\CSCI 4443\\Project\\chromedriver_win32\\chromedriver.exe\")\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    # Load the Goodreads author webpage\n",
    "    url = r.url\n",
    "    driver.get(url)\n",
    "\n",
    "    # Get the page source and parse it with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Close the Selenium driver\n",
    "    driver.quit()\n",
    "\n",
    "    return(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## grabs user information while there is a link that works properly and the page is one of the acceptable conditons for users\n",
    "\n",
    "def getUserInformation(linklst):\n",
    "    while True: \n",
    "        # time.sleep(0.5)\n",
    "        # clear_output()\n",
    "        try:\n",
    "            # intializing tag to search for with spamRequestsFind to verify webpage html was successfully retrieved\n",
    "            # tag is chosen as something common to any goodreads page, the siteHeader\n",
    "            tag = 'div'\n",
    "            attr = 'class'\n",
    "            attr_id = 'siteHeader'\n",
    "\n",
    "            # creating empty lists to house scraped information\n",
    "            user_ID = []\n",
    "            Display_Name = []\n",
    "            num_ratings = []\n",
    "            avg_rating = []\n",
    "            num_reviews = []\n",
    "            booklst = []\n",
    "            isAuthor = []\n",
    "\n",
    "\n",
    "            for i, x in enumerate(linklst):\n",
    "\n",
    "                soup, r = spamRequestsFind(x, tag, attr, attr_id)\n",
    "\n",
    "                user_ID += [re.sub(r'\\D', '', x)]\n",
    "\n",
    "                # executing scrape for goodreads author page\n",
    "                if soup.find('h3', attrs = {'class' : 'right goodreadsAuthor'}) != None:\n",
    "\n",
    "                    Display_Name += [soup.find('h1', attrs = {'class' : 'authorName'}).text.strip()]\n",
    "\n",
    "                    userInfo = soup.find('div', attrs = {'class' : 'smallText'}).find_all('a')\n",
    "                    num_ratings += [re.sub(r'[^\\d.]', '', userInfo[0].text)]\n",
    "                    avg_rating += [re.sub(r'[^\\d.]', '', userInfo[2].text)]\n",
    "                    num_reviews += [re.sub(r'[^\\d.]', '', userInfo[1].text)]\n",
    "                    booklst += [None]\n",
    "                    isAuthor += ['yes']\n",
    "                \n",
    "                # executing scrape for user whose href has changed after becoming a goodreads verified author\n",
    "                elif r.url != x and soup.find('h3', attrs = {'class' : 'right goodreadsAuthor'}) != None:\n",
    "\n",
    "                    soup = updated_url_html(r)\n",
    "\n",
    "                    Display_Name += [soup.find('h1', attrs = {'class' : 'authorName'}).text.strip()]\n",
    "\n",
    "                    userInfo = soup.find('div', attrs = {'class' : 'smallText'}).find_all('a')\n",
    "                    num_ratings += [re.sub(r'[^\\d.]', '', userInfo[0].text)]\n",
    "                    avg_rating += [re.sub(r'[^\\d.]', '', userInfo[2].text)]\n",
    "                    num_reviews += [re.sub(r'[^\\d.]', '', userInfo[1].text)]\n",
    "                    booklst += [None]\n",
    "                    isAuthor += ['yes']\n",
    "\n",
    "                # executing scrape for normal user account\n",
    "                elif soup.find('h1', attrs = {'class' : 'userProfileName'}) != None:\n",
    "\n",
    "                    # Retrieving the name of the user\n",
    "                    Display_Name += [soup.find('h1', attrs = {'class' : 'userProfileName'}).text.strip()]\n",
    "\n",
    "                    # Retrieving userInfo element that houses a tags with information of user total reviews and ratings \n",
    "                    userInfo = soup.find('div', attrs = {'class' : 'profilePageUserStatsInfo'}).find_all('a')\n",
    "                    num_ratings += [re.sub(r'[^\\d.]', '', userInfo[0].text)]\n",
    "                    avg_rating += [re.sub(r'[^\\d.]', '', userInfo[1].text)]\n",
    "                    num_reviews += [re.sub(r'[^\\d.]', '', userInfo[2].text.strip())]\n",
    "\n",
    "                    # Retrieving up to 10 of users favorite books\n",
    "                    if soup.find('div', attrs = {'class' : 'imgGrid'}) != None:\n",
    "                        favBooks = soup.find('div', attrs = {'class' : 'imgGrid'}).find_all('img')\n",
    "                        books = []\n",
    "                        for i,x in enumerate(favBooks):\n",
    "                            books += [favBooks[i]['title']]\n",
    "                        booklst += [books]\n",
    "                    else:\n",
    "                        booklst += [None]\n",
    "                    \n",
    "                    isAuthor += ['no']\n",
    "\n",
    "                # executing for private user who has changed their display name\n",
    "                elif r.url != x and soup.find('div', attrs = {'class' : 'mainContentFloat'}) != None:\n",
    "\n",
    "                    soup = updated_url_html(r)\n",
    "\n",
    "                    # Retrieving the name of the user\n",
    "                    Display_Name += [soup.find('div', attrs = {'class' : 'mainContentFloat'}).find('h1').text.strip()]\n",
    "\n",
    "                    # Retrieving userInfo element that houses a tags with information of user total reviews and ratings \n",
    "                    userInfo = soup.find('div', attrs = {'class' : 'smallText'}).find_all('a')\n",
    "                    avg_rating += [re.sub(r'[^\\d.]', '', userInfo[0].text)]\n",
    "\n",
    "                    # text order is 'total_ratings|total_reviews:avg_rating'\n",
    "                    cleaner_info = re.sub(r'[^0-9:|]', '', soup.find('div', attrs = {'class' : 'smallText'}).text.strip().replace('\\n', '').replace('\\t', ''))\n",
    "                    num_ratings += [cleaner_info.split('|',1)[0]]\n",
    "                    num_reviews += [re.findall(r'\\|(.*?)\\:', cleaner_info)[0]]\n",
    "\n",
    "                    # Cannot see favorited books due to privacy settings\n",
    "                    booklst += [None]\n",
    "\n",
    "                    isAuthor += ['no']\n",
    "                \n",
    "                # exceuting for page that has been deleted\n",
    "                elif soup.find('h4', attrs = {'class' : 'gr-h4'}) != None:\n",
    "                    Display_Name += ['DELETED']\n",
    "                    avg_rating += [None]\n",
    "                    num_ratings += [None]\n",
    "                    num_reviews += [None]\n",
    "                    booklst += [None]\n",
    "                    isAuthor += [None]\n",
    "\n",
    "                # executing scrape for private user account\n",
    "                else:\n",
    "\n",
    "                    # Retrieving the name of the user\n",
    "                    Display_Name += [soup.find('div', attrs = {'class' : 'mainContentFloat'}).find('h1').text.strip()]\n",
    "\n",
    "                    # Retrieving userInfo element that houses a tags with information of user total reviews and ratings \n",
    "                    userInfo = soup.find('div', attrs = {'class' : 'smallText'}).find_all('a')\n",
    "                    avg_rating += [re.sub(r'[^\\d.]', '', userInfo[0].text)]\n",
    "\n",
    "                    # text order is 'total_ratings|total_reviews:avg_rating'\n",
    "                    cleaner_info = re.sub(r'[^0-9:|]', '', soup.find('div', attrs = {'class' : 'smallText'}).text.strip().replace('\\n', '').replace('\\t', ''))\n",
    "                    num_ratings += [cleaner_info.split('|',1)[0]]\n",
    "                    num_reviews += [re.findall(r'\\|(.*?)\\:', cleaner_info)[0]]\n",
    "\n",
    "                    # Cannot see favorited books due to privacy settings\n",
    "                    booklst += [None]\n",
    "\n",
    "                    isAuthor += ['no']\n",
    "\n",
    "                # print('{} iter successful, {} left in run'.format(len(Display_Name), len(linklst)-len(Display_Name)))\n",
    "                          \n",
    "        except (AttributeError):\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "\n",
    "    reviewData = pd.DataFrame({ 'User ID' : user_ID,\n",
    "                                'Display Name' : Display_Name,\n",
    "                                'Average Rating' : avg_rating,\n",
    "                                'Total Ratings' : num_ratings,\n",
    "                                'Total Reviews' : num_reviews,\n",
    "                                'Favorite Books' : booklst,   \n",
    "                                'Author' : isAuthor            })\n",
    "\n",
    "\n",
    "    return(reviewData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59881"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Appending current dataset to overall dataset\n",
    "# fulldata = pd.read_excel(\"UserInfoBank.xlsx\")\n",
    "# agg = pd.read_excel(\"userInfo_9.xlsx\")\n",
    "# comb_data = pd.concat([fulldata, agg], axis = 0)\n",
    "\n",
    "# comb_data.to_csv('UserInfoBank.csv', index=False, header=True)\n",
    "# comb_data.to_excel('UserInfoBank.xlsx', index=False, header=True)\n",
    "\n",
    "userIB = pd.read_csv('UserInfoBank.csv')\n",
    "len_so_far = len(userIB)\n",
    "len_so_far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appendLoop functions works to grab n amount of users info based on the current length of the dataset being appended and overall dataset collected\n",
    "# using multiple files encourages redundancy in the event a file is lost in iteration due to OS ERROR\n",
    "# Therefore, it will reduce this possibility and allow for a backup file to exist in the event\n",
    "\n",
    "def appendLoop(niters, len_so_far):\n",
    "    \n",
    "    for i in range(niters):\n",
    "        itr = pd.read_csv(\"userInfo_9.csv\")\n",
    "        start = len_so_far + len(itr) \n",
    "        end = start + 20\n",
    "        linklst = br_u[start:end]\n",
    "\n",
    "        revData = getUserInformation(linklst)\n",
    "        #revData.to_csv(\"userInfo_9.csv\", index=False, header=True)\n",
    "        revData.to_csv(\"userInfo_9.csv\", index=False, mode=\"a\", header=False)\n",
    "        #print('{} iterations successful, {} iterations left'.format(i+1, niters-(i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "appendLoop(10, len_so_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Dataset : 59881\n",
      "Unique Users in Dataset : 59881\n",
      "Length of current Dataset : 7986\n",
      "\t\n",
      "You're doing great bud\n"
     ]
    }
   ],
   "source": [
    "# confirming that total dataset (from combining current set to overall set) contains only unique users and no duplicates\n",
    "# then saving a .xlsx backup file for the current dataset csv being appended to \n",
    "\n",
    "def sanityCheck(comb_data):\n",
    "    uI9 = pd.read_csv(\"userInfo_9.csv\")\n",
    "    itr = pd.concat([comb_data, uI9], axis = 0)\n",
    "    print('Length of Dataset : {}'.format(len(itr)))\n",
    "    print('Unique Users in Dataset : {}'.format(len(itr['User ID'].unique())))\n",
    "    print('Length of current Dataset : {}'.format(len(uI9)))\n",
    "\n",
    "    print('\\t')\n",
    "\n",
    "    if len(itr) == len(itr['User ID'].unique()):\n",
    "        print(\"You're doing great bud\")\n",
    "    else:\n",
    "        print(\"Hey, I think we should talk...\")\n",
    "\n",
    "    uI9.to_excel('userInfo_9.xlsx', index=False)\n",
    "\n",
    "sanityCheck(userIB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # converting the UserInfo dataframe to parquet\n",
    "# pip install pyarrow\n",
    "\n",
    "# additionally, creating a compressed version of the file using lz4 in a parquet file format\n",
    "# this turns out to actually be larger than a simple .xlsx of the case of a small dataset\n",
    "\n",
    "userIB.to_parquet('UserInfo.parquet', compression='lz4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
