{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from random import choice\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59881"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loading review data\n",
    "br = pd.read_csv('BookReviews.csv').dropna()\n",
    "br = br.reset_index(drop=True)\n",
    "\n",
    "## creating numpy array of review data with only unique user href's\n",
    "br_u = br['User Href'].unique()\n",
    "len(br_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining function to continuing spamming requests until html can be processed\n",
    "## when looking to iterate through multiple links while scraping\n",
    "\n",
    "# returns a soup and request\n",
    "# \n",
    "# The caveat is that by the definition, the function must find some object \n",
    "# that exists on the page in order to work \n",
    "\n",
    "def spamRequestsFind(link, tag, attr, attr_id):\n",
    "    status = None\n",
    "    while status == None:\n",
    "        r = requests.get(link)\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "        status = type(soup.find(tag, attrs = {attr : attr_id}))\n",
    "\n",
    "    prefix = len('https://www.goodreads.com/user/show/')\n",
    "    userID_disp = link[prefix:]\n",
    "    match = re.match(r'^(\\d+)-', userID_disp)\n",
    "    if match:\n",
    "        userID = match.group(1)\n",
    "    return(soup, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updated_url_html(r):\n",
    "    # Set up the Selenium driver with the path to the ChromeDriver executable\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    service = Service(r\"C:\\Users\\marty\\OneDrive - The George Washington University\\Documents\\CSCI 4443\\Project\\chromedriver_win32\\chromedriver.exe\")\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    # Load the Goodreads author webpage\n",
    "    url = r.url\n",
    "    driver.get(url)\n",
    "\n",
    "    # Get the page source and parse it with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Close the Selenium driver\n",
    "    driver.quit()\n",
    "\n",
    "    return(soup)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 normal user\n",
      "41 author\n",
      "42 author\n",
      "43 normal user\n",
      "44 normal user\n",
      "45 normal user\n",
      "46 private user\n",
      "47 private user\n",
      "48 normal user\n",
      "49 normal user\n"
     ]
    }
   ],
   "source": [
    "itr = pd.read_csv(\"userInfo.csv\")\n",
    "start = len(itr)\n",
    "end = start + 50\n",
    "\n",
    "linklst = br_u[start:end]\n",
    "\n",
    "\n",
    "def getUserInformation(linklst):\n",
    "    while True: \n",
    "        try:\n",
    "            # intializing tag to search for with spamRequestsFind to verify webpage html was successfully retrieved\n",
    "            # tag is chosen as something common to any goodreads page, the siteHeader\n",
    "            tag = 'div'\n",
    "            attr = 'class'\n",
    "            attr_id = 'siteHeader'\n",
    "\n",
    "            # creating empty lists to house scraped information\n",
    "            user_ID = []\n",
    "            Display_Name = []\n",
    "            num_ratings = []\n",
    "            avg_rating = []\n",
    "            num_reviews = []\n",
    "            booklst = []\n",
    "            isAuthor = []\n",
    "\n",
    "            for i, x in enumerate(linklst):\n",
    "\n",
    "                if i % 10 == 0:\n",
    "                    clear_output()\n",
    "\n",
    "                soup, r = spamRequestsFind(x, tag, attr, attr_id)\n",
    "\n",
    "                user_ID += [re.sub(r'\\D', '', x)]\n",
    "\n",
    "                # executing scrape for goodreads author page\n",
    "                if soup.find('h3', attrs = {'class' : 'right goodreadsAuthor'}) != None:\n",
    "                    print(i, 'author')\n",
    "\n",
    "                    Display_Name += [soup.find('h1', attrs = {'class' : 'authorName'}).text.strip()]\n",
    "\n",
    "                    userInfo = soup.find('div', attrs = {'class' : 'smallText'}).find_all('a')\n",
    "                    num_ratings += [re.sub(r'[^\\d.]', '', userInfo[0].text)]\n",
    "                    avg_rating += [re.sub(r'[^\\d.]', '', userInfo[2].text)]\n",
    "                    num_reviews += [re.sub(r'[^\\d.]', '', userInfo[1].text)]\n",
    "                    booklst += [None]\n",
    "                    isAuthor += ['yes']\n",
    "                \n",
    "                # executing scrape for user whose href has changed after becoming a goodreads verified author\n",
    "                elif r.url != x and soup.find('h3', attrs = {'class' : 'right goodreadsAuthor'}) != None:\n",
    "                    print(i, 'normal user to author')\n",
    "\n",
    "                    soup = updated_url_html(r)\n",
    "\n",
    "                    Display_Name += [soup.find('h1', attrs = {'class' : 'authorName'}).text.strip()]\n",
    "\n",
    "                    userInfo = soup.find('div', attrs = {'class' : 'smallText'}).find_all('a')\n",
    "                    num_ratings += [re.sub(r'[^\\d.]', '', userInfo[0].text)]\n",
    "                    avg_rating += [re.sub(r'[^\\d.]', '', userInfo[2].text)]\n",
    "                    num_reviews += [re.sub(r'[^\\d.]', '', userInfo[1].text)]\n",
    "                    booklst += [None]\n",
    "                    isAuthor += ['yes']\n",
    "\n",
    "                # executing scrape for normal user account\n",
    "                elif soup.find('h1', attrs = {'class' : 'userProfileName'}) != None:\n",
    "                    print(i, 'normal user')\n",
    "\n",
    "                    # Retrieving the name of the user\n",
    "                    Display_Name += [soup.find('h1', attrs = {'class' : 'userProfileName'}).text.strip()]\n",
    "\n",
    "                    # Retrieving userInfo element that houses a tags with information of user total reviews and ratings \n",
    "                    userInfo = soup.find('div', attrs = {'class' : 'profilePageUserStatsInfo'}).find_all('a')\n",
    "                    num_ratings += [re.sub(r'[^\\d.]', '', userInfo[0].text)]\n",
    "                    avg_rating += [re.sub(r'[^\\d.]', '', userInfo[1].text)]\n",
    "                    num_reviews += [re.sub(r'[^\\d.]', '', userInfo[2].text.strip())]\n",
    "\n",
    "                    # Retrieving up to 10 of users favorite books\n",
    "                    if soup.find('div', attrs = {'class' : 'imgGrid'}) != None:\n",
    "                        favBooks = soup.find('div', attrs = {'class' : 'imgGrid'}).find_all('img')\n",
    "                        books = []\n",
    "                        for i,x in enumerate(favBooks):\n",
    "                            books += [favBooks[i]['title']]\n",
    "                        booklst += [books]\n",
    "                    else:\n",
    "                        booklst += [None]\n",
    "                    \n",
    "                    isAuthor += ['no']\n",
    "\n",
    "                # executing for private user who has changed their display name\n",
    "                elif r.url != x and soup.find('div', attrs = {'class' : 'mainContentFloat'}) != None:\n",
    "                    print(i, 'private, name change')\n",
    "\n",
    "                    soup = updated_url_html(r)\n",
    "\n",
    "                    # Retrieving the name of the user\n",
    "                    Display_Name += [soup.find('div', attrs = {'class' : 'mainContentFloat'}).find('h1').text.strip()]\n",
    "\n",
    "                    # Retrieving userInfo element that houses a tags with information of user total reviews and ratings \n",
    "                    userInfo = soup.find('div', attrs = {'class' : 'smallText'}).find_all('a')\n",
    "                    avg_rating += [re.sub(r'[^\\d.]', '', userInfo[0].text)]\n",
    "\n",
    "                    # text order is 'total_ratings|total_reviews:avg_rating'\n",
    "                    cleaner_info = re.sub(r'[^0-9:|]', '', soup.find('div', attrs = {'class' : 'smallText'}).text.strip().replace('\\n', '').replace('\\t', ''))\n",
    "                    num_ratings += [cleaner_info.split('|',1)[0]]\n",
    "                    num_reviews += [re.findall(r'\\|(.*?)\\:', cleaner_info)[0]]\n",
    "\n",
    "                    # Cannot see favorited books due to privacy settings\n",
    "                    booklst += [None]\n",
    "\n",
    "                    isAuthor += ['no']\n",
    "\n",
    "                # executing scrape for private user account\n",
    "                else:\n",
    "                    print(i, 'private user')\n",
    "\n",
    "                    # Retrieving the name of the user\n",
    "                    Display_Name += [soup.find('div', attrs = {'class' : 'mainContentFloat'}).find('h1').text.strip()]\n",
    "\n",
    "                    # Retrieving userInfo element that houses a tags with information of user total reviews and ratings \n",
    "                    userInfo = soup.find('div', attrs = {'class' : 'smallText'}).find_all('a')\n",
    "                    avg_rating += [re.sub(r'[^\\d.]', '', userInfo[0].text)]\n",
    "\n",
    "                    # text order is 'total_ratings|total_reviews:avg_rating'\n",
    "                    cleaner_info = re.sub(r'[^0-9:|]', '', soup.find('div', attrs = {'class' : 'smallText'}).text.strip().replace('\\n', '').replace('\\t', ''))\n",
    "                    num_ratings += [cleaner_info.split('|',1)[0]]\n",
    "                    num_reviews += [re.findall(r'\\|(.*?)\\:', cleaner_info)[0]]\n",
    "\n",
    "                    # Cannot see favorited books due to privacy settings\n",
    "                    booklst += [None]\n",
    "\n",
    "                    isAuthor += ['no']\n",
    "\n",
    "        except (AttributeError):\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "\n",
    "    reviewData = pd.DataFrame({ 'User ID' : user_ID,\n",
    "                                'Display Name' : Display_Name,\n",
    "                                'Average Rating' : avg_rating,\n",
    "                                'Total Ratings' : num_ratings,\n",
    "                                'Total Reviews' : num_reviews,\n",
    "                                'Favorite Books' : booklst,   \n",
    "                                'Author' : isAuthor            })\n",
    "\n",
    "\n",
    "    return(reviewData)\n",
    "\n",
    "revData = getUserInformation(linklst)\n",
    "revData.to_csv(\"userInfo.csv\", index=False, mode=\"a\", header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itr = pd.read_csv(\"userInfo.csv\")\n",
    "itr[start:end]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
